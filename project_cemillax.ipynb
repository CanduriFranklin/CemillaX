{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwlntUILoiMSvhuv2/EuWe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CanduriFranklin/CemillaX/blob/main/project_cemillax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Challenge to develop a machine learning model to predict heat island hotspots in an urban location. Furthermore, the model should be designed to discern and highlight the key factors that significantly contribute to the development of these hotspots within urban environments."
      ],
      "metadata": {
        "id": "8BX-rJrvTSwR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Design, develop, train and implement a \"Random Forest\" model."
      ],
      "metadata": {
        "id": "ApnBh9jIUETh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Structure for Organizing the Notebook\n",
        "Organization of the notebook as follows:\n",
        "\n",
        "1.- Loading and Processing Data.\n",
        "\n",
        "2.- Model Training.\n",
        "\n",
        "3.- Model Evaluation.\n",
        "\n",
        "4.- Model Interpretation (SHAP).\n",
        "\n",
        "5.- Model Saving.\n"
      ],
      "metadata": {
        "id": "LQn88h97GVHO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dependencies (Libraries)"
      ],
      "metadata": {
        "id": "m5bp3EMgJD-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision pandas numpy opencv-python scikit-learn matplotlib seaborn"
      ],
      "metadata": {
        "id": "g8hg4YfznOQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.- Data Loading and Processing"
      ],
      "metadata": {
        "id": "S3LaGQRBJV14"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openpyxl\n",
        "import pandas as pd\n",
        "\n",
        "def load_and_process_data(file_path):\n",
        "    \"\"\"\n",
        "    Load and process data from an Excel file.\n",
        "\n",
        "    Parameters:\n",
        "    file_path (str): Path to the Excel file in Google Colab.\n",
        "\n",
        "    Returns:\n",
        "    df_model (DataFrame): Processed DataFrame with the features and the UHI.\n",
        "    \"\"\"\n",
        "    # Load the sheets from the Excel file\n",
        "    df_bronx = pd.read_excel(file_path, sheet_name='Bronx')\n",
        "    df_manhattan = pd.read_excel(file_path, sheet_name='Manhattan')\n",
        "\n",
        "    # Display the first rows of each DataFrame for verification\n",
        "    print(\"Bronx data:\")\n",
        "    print(df_bronx.head())\n",
        "\n",
        "    print(\"\\nManhattan data:\")\n",
        "    print(df_manhattan.head())\n",
        "\n",
        "    # Convert datetime column to datetime type\n",
        "    df_bronx['Date / Time'] = pd.to_datetime(df_bronx['Date / Time'])\n",
        "    df_manhattan['Date / Time'] = pd.to_datetime(df_manhattan['Date / Time'])\n",
        "\n",
        "    # Merge the data from the weather stations\n",
        "    df_combined = pd.merge(df_bronx, df_manhattan, on='Date / Time', how='left', suffixes=('_bronx', '_manhattan'))\n",
        "\n",
        "    # Calculate the UHI as the temperature difference between the Bronx and Manhattan\n",
        "    df_combined['UHI'] = df_combined['Air Temp at Surface [degC]_bronx'] - df_combined['Air Temp at Surface [degC]_manhattan']\n",
        "\n",
        "    # Calculate the Heat Index (heat index) for both seasons\n",
        "    def calculate_heat_index(temp, humidity):\n",
        "        # Simplified Heat Index formula\n",
        "        return temp + 0.5 * humidity\n",
        "\n",
        "    df_combined['Heat_Index_bronx'] = calculate_heat_index(df_combined['Air Temp at Surface [degC]_bronx'], df_combined['Relative Humidity [percent]_bronx'])\n",
        "    df_combined['Heat_Index_manhattan'] = calculate_heat_index(df_combined['Air Temp at Surface [degC]_manhattan'], df_combined['Relative Humidity [percent]_manhattan'])\n",
        "\n",
        "    # Select the relevant columns for the model\n",
        "    # Corrected feature names to remove the unwanted unicode characters\n",
        "    features = [\n",
        "        'Air Temp at Surface [degC]_bronx',\n",
        "        'Relative Humidity [percent]_bronx',\n",
        "        'Avg Wind Speed [m/s]_bronx',  # Corrected feature name\n",
        "        'Wind Direction [degrees]_bronx',\n",
        "        'Solar Flux [W/m^2]_bronx',\n",
        "        'Air Temp at Surface [degC]_manhattan',\n",
        "        'Relative Humidity [percent]_manhattan',\n",
        "        'Avg Wind Speed [m/s]_manhattan',  # Corrected feature name\n",
        "        'Wind Direction [degrees]_manhattan',\n",
        "        'Solar Flux [W/m^2]_manhattan',\n",
        "        'Heat_Index_bronx',\n",
        "        'Heat_Index_manhattan'\n",
        "    ]\n",
        "\n",
        "    # Create a DataFrame with the target features and variable (UHI)\n",
        "    df_model = df_combined[features + ['UHI']]\n",
        "\n",
        "    # Remove rows with missing values ​​(if any)\n",
        "    df_model.dropna(inplace=True)\n",
        "\n",
        "    return df_model\n",
        "\n",
        "# File path in Google Colab\n",
        "file_path = '/content/Training_data_uhi_index_2025-02-18.csv'\n",
        "\n",
        "# Load and process the data\n",
        "df_model = load_and_process_data(file_path)\n",
        "\n",
        "# Display the final DataFrame\n",
        "print(\"\\nFinal DataFrame for Model:\")\n",
        "print(df_model.head())\n",
        "\n",
        "# Save the processed DataFrame to a new Excel file\n",
        "output_file_path = '/content/NY_Mesonet_Weather_Processed.csv'\n",
        "df_model.to_excel(output_file_path, index=False)\n",
        "\n",
        "print(f\"\\nProcessed data saved in: {output_file_path}\")"
      ],
      "metadata": {
        "id": "wnQYrHiT2cyh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Model Training"
      ],
      "metadata": {
        "id": "gr436H8sJgDF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import unittest\n",
        "\n",
        "# Load the processed data\n",
        "df_model = pd.read_excel('/content/NY_Mesonet_Weather_Processed.xlsx')\n",
        "\n",
        "# Separate features (X) and target variable (y)\n",
        "X = df_model.drop('UHI', axis=1)\n",
        "y = df_model['UHI']\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the model\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "#Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "#Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f'MSE: {mse}')\n",
        "print(f'R²: {r2}')\n",
        "\n",
        "# Unit tests to verify the performance of the model.\n",
        "class TestModel(unittest.TestCase):\n",
        "    def test_model_training(self): # Corrected indentation\n",
        "        \"\"\"Verify that the model is trained correctly.\"\"\"\n",
        "        self.assertIsNotNone(model, \"The model was not trained correctly.\")\n",
        "\n",
        "    def test_model_predictions(self): # Corrected indentation\n",
        "        \"\"\"Verify that the model makes predictions.\"\"\"\n",
        "        predictions = model.predict(X_test)\n",
        "        self.assertEqual(len(predictions), len(y_test), \"The number of predictions does not match the number of tests.\")\n",
        "\n",
        "    def test_model_performance(self): # Corrected indentation\n",
        "        \"\"\"Verify that the model's performance is acceptable.\"\"\"\n",
        "        self.assertGreater(r2, 0.5, f\"The R² is too low: {r2}\")\n",
        "        self.assertLess(mse, 10, f\"The MSE is too high: {mse}\")\n",
        "\n",
        "#Run unit tests\n",
        "if __name__ == '__main__':\n",
        "    unittest.main(argv=[''], exit=False) # Corrected indentation, fixed call to unittest.main()\n",
        "\n",
        "#Run unit tests\n",
        "if __name__ == '__main__':\n",
        "    unittest.main(argv=[''], exit=False)\n",
        "df_model = pd.read_excel('/content/NY_Mesonet_Weather_Processed.xlsx')\n",
        "\n",
        "# Separate features (X) and target variable (y)\n",
        "X = df_model.drop('UHI', axis=1)\n",
        "y = df_model['UHI']\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train the model\n",
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "#Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "#Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f'MSE: {mse}')\n",
        "print(f'R²: {r2}')\n",
        "\n",
        "# Unit tests to verify the performance of the model.\n",
        "class TestModel(unittest.TestCase): # Corrected indentation\n",
        "    def test_model_training(self): # Corrected indentation, changed 'yo' to 'self'\n",
        "        \"\"\"Verify that the model is trained correctly.\"\"\"\n",
        "        self.assertIsNotNone(model, \"The model was not trained correctly.\")\n",
        "\n",
        "    def test_model_predictions(self): # Corrected indentation, changed 'yo' to 'self'\n",
        "        \"\"\"Verify that the model makes predictions.\"\"\"\n",
        "        predictions = model.predict(X_test)\n",
        "        self.assertEqual(len(predictions), len(y_test), \"The number of predictions does not match the number of tests.\")\n",
        "\n",
        "    def test_model_performance(self): # Corrected indentation, changed 'yo' to 'self'\n",
        "        \"\"\"Verify that the model's performance is acceptable.\"\"\"\n",
        "        self.assertGreater(r2, 0.5, f\"The R² is too low: {r2}\")\n",
        "        self.assertLess(mse, 10, f\"The MSE is too high: {mse}\")\n",
        "\n",
        "#Run unit tests\n",
        "if __name__ == '__main__':\n",
        "    unittest.main(argv=[''], exit=False) # Corrected indentation, fixed call to unittest.main(), removed output argument"
      ],
      "metadata": {
        "id": "-gwJo1B74BxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Model Interpretation (SHAP)"
      ],
      "metadata": {
        "id": "V7-Kyy01JyVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to interpret the model with SHAP\n",
        "!pip install shap\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import shap\n",
        "\n",
        "# 1. Feature Importance\n",
        "importances = model.feature_importances_\n",
        "feature_names = X.columns\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(range(len(importances)), importances, align='center')\n",
        "plt.yticks(np.arange(len(importances)), feature_names)\n",
        "plt.xlabel('Feature Importance')\n",
        "plt.title('Feature Importance in the Model')\n",
        "plt.show()\n",
        "\n",
        "# Code to interpret the model with SHAP\n",
        "!pip install shap\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import shap\n",
        "\n",
        "# 1. Feature Importance\n",
        "importances = model.feature_importances_\n",
        "feature_names = X.columns\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(range(len(importances)), importances, align='center')\n",
        "plt.yticks(np.arange(len(importances)), feature_names)\n",
        "plt.xlabel('Feature Importance')\n",
        "plt.title('Feature Importance in the Model')\n",
        "plt.show()\n",
        "\n",
        "# 2. SHAP Values\n",
        "explainer = shap.TreeExplainer(model)\n",
        "shap_values = explainer.shap_values(X_train)\n",
        "\n",
        "# Plot feature importance\n",
        "shap.summary_plot(shap_values, X_train)\n",
        "\n",
        "# 3. Partial Dependency (example with one feature)\n",
        "shap.dependence_plot('Air Temp at Surface [degC]_bronx', shap_values, X_train)\n",
        "shap.summary_plot(shap_values, X_train)\n",
        "\n",
        "# 3. Partial Dependency (example with one feature)\n",
        "shap.dependence_plot('Air Temp at Surface [degC]_bronx', shap_values, X_train)"
      ],
      "metadata": {
        "id": "xN0NJNFgFiTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Saving the Model"
      ],
      "metadata": {
        "id": "ixiqfnNXJ7yP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code to save the model\n",
        "import joblib\n",
        "\n",
        "# Save the model\n",
        "joblib.dump(model, 'cemillax_uhi_prediction_model.pkl')\n",
        "\n",
        "print(\"Model saved as 'cemillax_uhi_prediction_model.pkl'.\")\n",
        "\n",
        "# Load the model (optional, for verification)\n",
        "model = joblib.load('cemillax_uhi_prediction_model.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKM3UWzVFjS7",
        "outputId": "01d996f0-f091-444c-f66f-508ddb9f56d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved as 'cemillax_uhi_prediction_model.pkl'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Intermediate Results:"
      ],
      "metadata": {
        "id": "fVmf36sxXrUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.savefig('importance characteristics.png')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "VniysjzqXw-8",
        "outputId": "6890d120-bd16-4546-ca8d-279f46e0cbdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}